{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f90d4fe7-76d2-43d9-8c9c-522a0282fec7",
   "metadata": {},
   "source": [
    "<p>\n",
    "    ### **Q1. What is the Filter Method in Feature Selection, and How Does It Work?**\n",
    "The **Filter Method** is a feature selection technique that evaluates the relevance of input features based on statistical measures without using a predictive model. It removes irrelevant or redundant features before training the model.\n",
    "\n",
    "**How It Works:**\n",
    "1. Computes statistical scores between each feature and the target variable.\n",
    "2. Ranks the features based on correlation, mutual information, or other statistical metrics.\n",
    "3. Selects the top-ranked features for model training.\n",
    "\n",
    "**Common Techniques:**\n",
    "- Pearson Correlation\n",
    "- Chi-square Test\n",
    "- Mutual Information\n",
    "- ANOVA (Analysis of Variance)\n",
    "\n",
    "---\n",
    "\n",
    "### **Q2. How Does the Wrapper Method Differ from the Filter Method in Feature Selection?**\n",
    "| Aspect | Filter Method | Wrapper Method |\n",
    "|--------|-------------|---------------|\n",
    "| **Approach** | Uses statistical tests to rank features | Uses a predictive model to evaluate feature subsets |\n",
    "| **Computational Cost** | Fast and efficient | Computationally expensive |\n",
    "| **Model Dependency** | Independent of the model | Model-dependent |\n",
    "| **Example Techniques** | Correlation, Mutual Information | Recursive Feature Elimination (RFE), Forward/Backward Selection |\n",
    "\n",
    "The **Wrapper Method** searches for the best feature subset by training multiple models and evaluating performance.\n",
    "\n",
    "---\n",
    "\n",
    "### **Q3. What Are Some Common Techniques Used in Embedded Feature Selection Methods?**\n",
    "Embedded methods perform feature selection **during** the model training process. Common techniques include:\n",
    "\n",
    "1. **Lasso (L1 Regularization)** – Shrinks less important feature coefficients to zero.\n",
    "2. **Ridge (L2 Regularization)** – Reduces feature importance without eliminating them.\n",
    "3. **Decision Tree Feature Importance** – Uses Gini Impurity or Entropy to rank features.\n",
    "4. **Gradient Boosting Feature Selection** – Feature importance derived from boosting models like XGBoost.\n",
    "\n",
    "---\n",
    "\n",
    "### **Q4. What Are Some Drawbacks of Using the Filter Method for Feature Selection?**\n",
    "- **Ignores Feature Interactions:** Evaluates features individually without considering relationships between them.\n",
    "- **Not Model-Specific:** The selected features may not be optimal for a particular model.\n",
    "- **May Select Irrelevant Features:** Some statistically significant features may not contribute meaningfully to model performance.\n",
    "\n",
    "---\n",
    "\n",
    "### **Q5. In Which Situations Would You Prefer Using the Filter Method Over the Wrapper Method?**\n",
    "Use the **Filter Method** when:\n",
    "- You have a **large dataset** with many features.\n",
    "- You need a **fast and computationally efficient** method.\n",
    "- The features are **mostly independent**, reducing the need for model-specific selection.\n",
    "- You want a **generalized feature selection** that can be used across different models.\n",
    "\n",
    "---\n",
    "\n",
    "### **Q6. Selecting Features for Customer Churn Prediction Using the Filter Method**\n",
    "**Steps:**\n",
    "1. **Data Preprocessing:** Handle missing values, encode categorical data.\n",
    "2. **Correlation Analysis:** Remove highly correlated features (e.g., Pearson correlation).\n",
    "3. **Chi-square Test:** Identify the categorical features most relevant to churn.\n",
    "4. **Mutual Information:** Measure the dependency between features and churn labels.\n",
    "5. **Feature Ranking:** Select top-ranked features for model training.\n",
    "\n",
    "---\n",
    "\n",
    "### **Q7. Selecting Features for Soccer Match Prediction Using the Embedded Method**\n",
    "**Steps:**\n",
    "1. **Train a Decision Tree Model** – Extract feature importance scores.\n",
    "2. **Apply Lasso (L1 Regularization)** – Remove less important features.\n",
    "3. **Use Feature Importance from XGBoost or Random Forest** – Select highly impactful features.\n",
    "4. **Iterate and Optimize** – Fine-tune the feature selection process based on model performance.\n",
    "\n",
    "---\n",
    "\n",
    "### **Q8. Selecting Features for House Price Prediction Using the Wrapper Method**\n",
    "**Steps:**\n",
    "1. **Train a Baseline Model** – Use all features and evaluate performance.\n",
    "2. **Recursive Feature Elimination (RFE):**\n",
    "   - Start with all features.\n",
    "   - Iteratively remove the least important feature.\n",
    "   - Retrain the model and evaluate.\n",
    "3. **Forward/Backward Selection:**\n",
    "   - Add or remove features one by one.\n",
    "   - Check model performance at each step.\n",
    "4. **Select the Best Subset** – Based on validation accuracy and model complexity.\n",
    "\n",
    "---\n",
    "\n",
    "### **Final Steps:**\n",
    "1. **Implement these solutions in Python using `scikit-learn`.**\n",
    "2. **Upload your Jupyter Notebook to GitHub.**\n",
    "3. **Submit the public repository link as required.**</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40522565-3fa4-4d26-a913-ed917499c749",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:base] *",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
